{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx6WegpLi_LS",
        "outputId": "7bbf8858-58f1-4a83-9d61-38747fee52d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-24 09:54:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-07-24 09:54:18 (21.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBQozQ_FuEtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "uWdbL-JijQdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters:\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7cm_tkpjb1P",
        "outputId": "1bc31df9-51f5-4bbb-dc5d-8bffd7172c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0--OGtYjjmPe",
        "outputId": "92023901-62fc-40f8-ace0-303afd86b786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhmWzi5jjrMC",
        "outputId": "ffcfb805-ece0-47cc-c1bc-40a3bcaeb82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenize --- convert raw text to some sequence of integers according to some vocabulary\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s:[stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hi there\"))\n",
        "print(decode(encode(\"hi there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fITw6-xLkCr0",
        "outputId": "e32d1287-5584-43d4-aa84-150403a993f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 1, 58, 46, 43, 56, 43]\n",
            "hi there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UetzwQk2hOHB",
        "outputId": "ca957bdd-5b04-4c76-e0e1-d32ea743062d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "loI-RkdahhQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjNQfAU0h2SN",
        "outputId": "b0b6d55d-55e0-4d8d-c135-0033b639c4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is:{target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jwAK2Wwh9Kr",
        "outputId": "15d9f490-6b0e-4201-a36f-df9e76893d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is:47\n",
            "when input is tensor([18, 47]) the target is:56\n",
            "when input is tensor([18, 47, 56]) the target is:57\n",
            "when input is tensor([18, 47, 56, 57]) the target is:58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is:1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is:15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is:47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is:58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size=4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split==\"train\" else val_data\n",
        "  ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----------')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X42kD-28im6V",
        "outputId": "4b98c2db-3b08-44e0-cb7b-662458cff4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----------\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Bigram model as a baseline"
      ],
      "metadata": {
        "id": "irTHVD5PvDOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Simple baseline model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 8 # what is the maximum context length for the predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate= 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is the (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits,loss = self(idx)\n",
        "      # forcus only on the last time step\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      # applend sampled index to the running sequene\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "out, loss = m(xb,yb)\n",
        "print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "id = torch.zeros((1,1), dtype= torch.long)\n",
        "print(decode(m.generate(id,max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "### Initial loss will be -ln(1/65) as we have 65 possible vocabulary elements\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "judppe4wnvzU",
        "outputId": "c3ce36e0-38f6-469e-9947-24b76c4fb039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(m.parameters(), lr= 1e-3)"
      ],
      "metadata": {
        "id": "haxcTP9kpmXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for steps in range(10000):\n",
        "  xb,yb = get_batch('train')\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_PSVwSssw7h",
        "outputId": "10ac97e4-490c-4a32-a84f-177b3e0b9a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.531221389770508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype= torch.long), max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI1vmswctZY-",
        "outputId": "68568030-4a36-4154-d16e-87db1984b109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tom ndiske\n",
            "tukistlinas:\n",
            "\n",
            "Orat lyowe wiclin:\n",
            "ALLars t th cinod as!\n",
            "Thevedevead;AEJUCaroftt,\n",
            "ADerde hy s tr;\n",
            "GSt cicoutrncolond p f IAwathaner:\n",
            "ONat they t fo be gs blle weamyoure itintet in'sut loomalll ink!\n",
            "F s sur,\n",
            "Thaveine thayouditelfrenevecyowe tes, hile\n",
            "Fas d,\n",
            "TID:\n",
            "\n",
            "\n",
            "OME sheve ind f bevk, nes k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention"
      ],
      "metadata": {
        "id": "4zAyg9K-vKSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1iX1HROvNAV",
        "outputId": "2d1bfc7c-c007-490c-d155-7f37b9e3e58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "## Hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256 # what is the maximum context length for the predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 500\n",
        "learning_rate= 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_layer =6\n",
        "n_head = 6\n",
        "dropout = 0.2\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) # (B,T,C)\n",
        "    q = self.query(x) # (B,T,C)\n",
        "    v = self.value(x) # (B,T,C)\n",
        "\n",
        "    # compute self-attention or affinities among the key and query\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1)  #(B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) #(B,T,C)\n",
        "    out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_embd : embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x)) # x + self.sa is the residual skip connections\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device= device)) # (T,C)\n",
        "    x = tok_emb + pos_emb # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is the (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits,loss = self(idx_cond)\n",
        "      # forcus only on the last time step\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      # applend sampled index to the running sequene\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel()\n",
        "out, loss = m(xb,yb)\n",
        "print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "id = torch.zeros((1,1), dtype= torch.long)\n",
        "print(decode(m.generate(id,max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "### Initial loss will be -ln(1/65) as we have 65 possible vocabulary elements\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHagMBuauFuv",
        "outputId": "3dc133a5-ae23-4da3-ee87-cf2fa320c191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.6739, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "y3,MS;KWohAJDg Z &Yo&LrnQjAEBolP'EAexQfm3GYfrn$Hk&iPomcXcfW iKPzM\n",
            "yUrupQl.Tpl..Wdd-KhIi.G arxcpAf dz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr= learning_rate)\n",
        "for steps in range(max_iters):\n",
        "  xb,yb = get_batch('train')\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Train loss at epoch {steps} = {loss.item()}\")\n",
        "  if steps%eval_interval == 0:\n",
        "    xv,yv = get_batch(\"val\")\n",
        "    logits, loss = m(xv,yv)\n",
        "    print(f\"val loss = {loss.item()}\" )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_voZ9KYEy4-j",
        "outputId": "cfc20b90-c2c2-4f14-e487-a2fb4a305752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss at epoch 0 = 4.526788234710693\n",
            "val loss = 3.90280818939209\n",
            "Train loss at epoch 1 = 3.8721046447753906\n",
            "Train loss at epoch 2 = 3.943657875061035\n",
            "Train loss at epoch 3 = 3.748080253601074\n",
            "Train loss at epoch 4 = 3.496762275695801\n",
            "Train loss at epoch 5 = 3.2048966884613037\n",
            "Train loss at epoch 6 = 3.2369601726531982\n",
            "Train loss at epoch 7 = 3.1424763202667236\n",
            "Train loss at epoch 8 = 3.053337335586548\n",
            "Train loss at epoch 9 = 3.0583434104919434\n",
            "Train loss at epoch 10 = 2.983870506286621\n",
            "Train loss at epoch 11 = 2.9792001247406006\n",
            "Train loss at epoch 12 = 2.9429593086242676\n",
            "Train loss at epoch 13 = 2.882878303527832\n",
            "Train loss at epoch 14 = 2.8609464168548584\n",
            "Train loss at epoch 15 = 2.877351760864258\n",
            "Train loss at epoch 16 = 2.818812847137451\n",
            "Train loss at epoch 17 = 2.823803424835205\n",
            "Train loss at epoch 18 = 2.7864596843719482\n",
            "Train loss at epoch 19 = 2.767024517059326\n",
            "Train loss at epoch 20 = 2.742643356323242\n",
            "Train loss at epoch 21 = 2.7793896198272705\n",
            "Train loss at epoch 22 = 2.741788625717163\n",
            "Train loss at epoch 23 = 2.7359354496002197\n",
            "Train loss at epoch 24 = 2.7234373092651367\n",
            "Train loss at epoch 25 = 2.661454200744629\n",
            "Train loss at epoch 26 = 2.6726834774017334\n",
            "Train loss at epoch 27 = 2.675889730453491\n",
            "Train loss at epoch 28 = 2.663159132003784\n",
            "Train loss at epoch 29 = 2.6547598838806152\n",
            "Train loss at epoch 30 = 2.639833688735962\n",
            "Train loss at epoch 31 = 2.6393637657165527\n",
            "Train loss at epoch 32 = 2.630910873413086\n",
            "Train loss at epoch 33 = 2.6474688053131104\n",
            "Train loss at epoch 34 = 2.626903772354126\n",
            "Train loss at epoch 35 = 2.6332943439483643\n",
            "Train loss at epoch 36 = 2.6175360679626465\n",
            "Train loss at epoch 37 = 2.620633602142334\n",
            "Train loss at epoch 38 = 2.5767977237701416\n",
            "Train loss at epoch 39 = 2.6085643768310547\n",
            "Train loss at epoch 40 = 2.585082769393921\n",
            "Train loss at epoch 41 = 2.596743583679199\n",
            "Train loss at epoch 42 = 2.5973849296569824\n",
            "Train loss at epoch 43 = 2.6097660064697266\n",
            "Train loss at epoch 44 = 2.5857620239257812\n",
            "Train loss at epoch 45 = 2.586200475692749\n",
            "Train loss at epoch 46 = 2.584059953689575\n",
            "Train loss at epoch 47 = 2.558525800704956\n",
            "Train loss at epoch 48 = 2.5911855697631836\n",
            "Train loss at epoch 49 = 2.569991111755371\n",
            "Train loss at epoch 50 = 2.580319881439209\n",
            "Train loss at epoch 51 = 2.545140266418457\n",
            "Train loss at epoch 52 = 2.5761048793792725\n",
            "Train loss at epoch 53 = 2.5844876766204834\n",
            "Train loss at epoch 54 = 2.5494565963745117\n",
            "Train loss at epoch 55 = 2.5619702339172363\n",
            "Train loss at epoch 56 = 2.5465784072875977\n",
            "Train loss at epoch 57 = 2.550117254257202\n",
            "Train loss at epoch 58 = 2.557454824447632\n",
            "Train loss at epoch 59 = 2.5444579124450684\n",
            "Train loss at epoch 60 = 2.5612387657165527\n",
            "Train loss at epoch 61 = 2.551600694656372\n",
            "Train loss at epoch 62 = 2.540555477142334\n",
            "Train loss at epoch 63 = 2.5390031337738037\n",
            "Train loss at epoch 64 = 2.536409378051758\n",
            "Train loss at epoch 65 = 2.560516119003296\n",
            "Train loss at epoch 66 = 2.5188069343566895\n",
            "Train loss at epoch 67 = 2.5150043964385986\n",
            "Train loss at epoch 68 = 2.540975332260132\n",
            "Train loss at epoch 69 = 2.5414633750915527\n",
            "Train loss at epoch 70 = 2.5248517990112305\n",
            "Train loss at epoch 71 = 2.5403294563293457\n",
            "Train loss at epoch 72 = 2.5552992820739746\n",
            "Train loss at epoch 73 = 2.523063898086548\n",
            "Train loss at epoch 74 = 2.535938262939453\n",
            "Train loss at epoch 75 = 2.5170700550079346\n",
            "Train loss at epoch 76 = 2.523383378982544\n",
            "Train loss at epoch 77 = 2.519375801086426\n",
            "Train loss at epoch 78 = 2.5356786251068115\n",
            "Train loss at epoch 79 = 2.5291178226470947\n",
            "Train loss at epoch 80 = 2.538313627243042\n",
            "Train loss at epoch 81 = 2.4970104694366455\n",
            "Train loss at epoch 82 = 2.5134241580963135\n",
            "Train loss at epoch 83 = 2.5254111289978027\n",
            "Train loss at epoch 84 = 2.539280414581299\n",
            "Train loss at epoch 85 = 2.515399932861328\n",
            "Train loss at epoch 86 = 2.5236124992370605\n",
            "Train loss at epoch 87 = 2.5034914016723633\n",
            "Train loss at epoch 88 = 2.5137603282928467\n",
            "Train loss at epoch 89 = 2.50642466545105\n",
            "Train loss at epoch 90 = 2.5276951789855957\n",
            "Train loss at epoch 91 = 2.515993118286133\n",
            "Train loss at epoch 92 = 2.5092012882232666\n",
            "Train loss at epoch 93 = 2.5221588611602783\n",
            "Train loss at epoch 94 = 2.5175771713256836\n",
            "Train loss at epoch 95 = 2.4995269775390625\n",
            "Train loss at epoch 96 = 2.5407447814941406\n",
            "Train loss at epoch 97 = 2.50001859664917\n",
            "Train loss at epoch 98 = 2.50260591506958\n",
            "Train loss at epoch 99 = 2.5082900524139404\n",
            "Train loss at epoch 100 = 2.491504669189453\n",
            "Train loss at epoch 101 = 2.4996402263641357\n",
            "Train loss at epoch 102 = 2.5223231315612793\n",
            "Train loss at epoch 103 = 2.4935684204101562\n",
            "Train loss at epoch 104 = 2.5167462825775146\n",
            "Train loss at epoch 105 = 2.4948418140411377\n",
            "Train loss at epoch 106 = 2.500215530395508\n",
            "Train loss at epoch 107 = 2.5033011436462402\n",
            "Train loss at epoch 108 = 2.501988410949707\n",
            "Train loss at epoch 109 = 2.502920150756836\n",
            "Train loss at epoch 110 = 2.4954535961151123\n",
            "Train loss at epoch 111 = 2.491189956665039\n",
            "Train loss at epoch 112 = 2.4863178730010986\n",
            "Train loss at epoch 113 = 2.4616100788116455\n",
            "Train loss at epoch 114 = 2.5153801441192627\n",
            "Train loss at epoch 115 = 2.4878201484680176\n",
            "Train loss at epoch 116 = 2.4809226989746094\n",
            "Train loss at epoch 117 = 2.5014078617095947\n",
            "Train loss at epoch 118 = 2.469440460205078\n",
            "Train loss at epoch 119 = 2.5022592544555664\n",
            "Train loss at epoch 120 = 2.5034072399139404\n",
            "Train loss at epoch 121 = 2.5027711391448975\n",
            "Train loss at epoch 122 = 2.4903244972229004\n",
            "Train loss at epoch 123 = 2.5030720233917236\n",
            "Train loss at epoch 124 = 2.4948086738586426\n",
            "Train loss at epoch 125 = 2.479509115219116\n",
            "Train loss at epoch 126 = 2.461035966873169\n",
            "Train loss at epoch 127 = 2.4846436977386475\n",
            "Train loss at epoch 128 = 2.5047593116760254\n",
            "Train loss at epoch 129 = 2.5043163299560547\n",
            "Train loss at epoch 130 = 2.495509386062622\n",
            "Train loss at epoch 131 = 2.49657940864563\n",
            "Train loss at epoch 132 = 2.4964373111724854\n",
            "Train loss at epoch 133 = 2.4975152015686035\n",
            "Train loss at epoch 134 = 2.4824612140655518\n",
            "Train loss at epoch 135 = 2.496056318283081\n",
            "Train loss at epoch 136 = 2.4745664596557617\n",
            "Train loss at epoch 137 = 2.475792646408081\n",
            "Train loss at epoch 138 = 2.474609136581421\n",
            "Train loss at epoch 139 = 2.484118938446045\n",
            "Train loss at epoch 140 = 2.491791009902954\n",
            "Train loss at epoch 141 = 2.4737327098846436\n",
            "Train loss at epoch 142 = 2.4954144954681396\n",
            "Train loss at epoch 143 = 2.4689929485321045\n",
            "Train loss at epoch 144 = 2.5098564624786377\n",
            "Train loss at epoch 145 = 2.462329149246216\n",
            "Train loss at epoch 146 = 2.477365255355835\n",
            "Train loss at epoch 147 = 2.488312005996704\n",
            "Train loss at epoch 148 = 2.484661102294922\n",
            "Train loss at epoch 149 = 2.4880282878875732\n",
            "Train loss at epoch 150 = 2.4897663593292236\n",
            "Train loss at epoch 151 = 2.4891610145568848\n",
            "Train loss at epoch 152 = 2.477313280105591\n",
            "Train loss at epoch 153 = 2.4455888271331787\n",
            "Train loss at epoch 154 = 2.4716498851776123\n",
            "Train loss at epoch 155 = 2.4544594287872314\n",
            "Train loss at epoch 156 = 2.4839141368865967\n",
            "Train loss at epoch 157 = 2.455294609069824\n",
            "Train loss at epoch 158 = 2.472097158432007\n",
            "Train loss at epoch 159 = 2.4754738807678223\n",
            "Train loss at epoch 160 = 2.483163833618164\n",
            "Train loss at epoch 161 = 2.481043815612793\n",
            "Train loss at epoch 162 = 2.4447834491729736\n",
            "Train loss at epoch 163 = 2.4660847187042236\n",
            "Train loss at epoch 164 = 2.4699723720550537\n",
            "Train loss at epoch 165 = 2.467998504638672\n",
            "Train loss at epoch 166 = 2.4523723125457764\n",
            "Train loss at epoch 167 = 2.482908248901367\n",
            "Train loss at epoch 168 = 2.4599549770355225\n",
            "Train loss at epoch 169 = 2.4641640186309814\n",
            "Train loss at epoch 170 = 2.4540441036224365\n",
            "Train loss at epoch 171 = 2.461272954940796\n",
            "Train loss at epoch 172 = 2.468597412109375\n",
            "Train loss at epoch 173 = 2.441685676574707\n",
            "Train loss at epoch 174 = 2.449110746383667\n",
            "Train loss at epoch 175 = 2.465585470199585\n",
            "Train loss at epoch 176 = 2.44469952583313\n",
            "Train loss at epoch 177 = 2.4537899494171143\n",
            "Train loss at epoch 178 = 2.4520275592803955\n",
            "Train loss at epoch 179 = 2.4497032165527344\n",
            "Train loss at epoch 180 = 2.442732810974121\n",
            "Train loss at epoch 181 = 2.4339122772216797\n",
            "Train loss at epoch 182 = 2.453582525253296\n",
            "Train loss at epoch 183 = 2.471203327178955\n",
            "Train loss at epoch 184 = 2.471369504928589\n",
            "Train loss at epoch 185 = 2.452699661254883\n",
            "Train loss at epoch 186 = 2.4499380588531494\n",
            "Train loss at epoch 187 = 2.45973539352417\n",
            "Train loss at epoch 188 = 2.442481279373169\n",
            "Train loss at epoch 189 = 2.4560422897338867\n",
            "Train loss at epoch 190 = 2.446042776107788\n",
            "Train loss at epoch 191 = 2.4323062896728516\n",
            "Train loss at epoch 192 = 2.4478981494903564\n",
            "Train loss at epoch 193 = 2.446810483932495\n",
            "Train loss at epoch 194 = 2.4434149265289307\n",
            "Train loss at epoch 195 = 2.4479339122772217\n",
            "Train loss at epoch 196 = 2.4364912509918213\n",
            "Train loss at epoch 197 = 2.441166400909424\n",
            "Train loss at epoch 198 = 2.4343888759613037\n",
            "Train loss at epoch 199 = 2.4309370517730713\n",
            "Train loss at epoch 200 = 2.431394338607788\n",
            "Train loss at epoch 201 = 2.4147095680236816\n",
            "Train loss at epoch 202 = 2.4328291416168213\n",
            "Train loss at epoch 203 = 2.4360885620117188\n",
            "Train loss at epoch 204 = 2.442579746246338\n",
            "Train loss at epoch 205 = 2.4277093410491943\n",
            "Train loss at epoch 206 = 2.430264711380005\n",
            "Train loss at epoch 207 = 2.4257166385650635\n",
            "Train loss at epoch 208 = 2.403761625289917\n",
            "Train loss at epoch 209 = 2.397799015045166\n",
            "Train loss at epoch 210 = 2.42516827583313\n",
            "Train loss at epoch 211 = 2.421576499938965\n",
            "Train loss at epoch 212 = 2.4162697792053223\n",
            "Train loss at epoch 213 = 2.3956542015075684\n",
            "Train loss at epoch 214 = 2.4015114307403564\n",
            "Train loss at epoch 215 = 2.4241676330566406\n",
            "Train loss at epoch 216 = 2.4117119312286377\n",
            "Train loss at epoch 217 = 2.4113810062408447\n",
            "Train loss at epoch 218 = 2.379110336303711\n",
            "Train loss at epoch 219 = 2.4087321758270264\n",
            "Train loss at epoch 220 = 2.399623155593872\n",
            "Train loss at epoch 221 = 2.389253854751587\n",
            "Train loss at epoch 222 = 2.385883331298828\n",
            "Train loss at epoch 223 = 2.400765895843506\n",
            "Train loss at epoch 224 = 2.393848419189453\n",
            "Train loss at epoch 225 = 2.4000000953674316\n",
            "Train loss at epoch 226 = 2.3801863193511963\n",
            "Train loss at epoch 227 = 2.379347085952759\n",
            "Train loss at epoch 228 = 2.3707118034362793\n",
            "Train loss at epoch 229 = 2.3788321018218994\n",
            "Train loss at epoch 230 = 2.371354818344116\n",
            "Train loss at epoch 231 = 2.376768112182617\n",
            "Train loss at epoch 232 = 2.3613882064819336\n",
            "Train loss at epoch 233 = 2.3765738010406494\n",
            "Train loss at epoch 234 = 2.3756957054138184\n",
            "Train loss at epoch 235 = 2.3440299034118652\n",
            "Train loss at epoch 236 = 2.3767752647399902\n",
            "Train loss at epoch 237 = 2.3602805137634277\n",
            "Train loss at epoch 238 = 2.338364601135254\n",
            "Train loss at epoch 239 = 2.3382346630096436\n",
            "Train loss at epoch 240 = 2.3437228202819824\n",
            "Train loss at epoch 241 = 2.348155975341797\n",
            "Train loss at epoch 242 = 2.3286173343658447\n",
            "Train loss at epoch 243 = 2.338743209838867\n",
            "Train loss at epoch 244 = 2.3261704444885254\n",
            "Train loss at epoch 245 = 2.3282344341278076\n",
            "Train loss at epoch 246 = 2.3274269104003906\n",
            "Train loss at epoch 247 = 2.3307628631591797\n",
            "Train loss at epoch 248 = 2.3247621059417725\n",
            "Train loss at epoch 249 = 2.346813917160034\n",
            "Train loss at epoch 250 = 2.311215400695801\n",
            "Train loss at epoch 251 = 2.3052425384521484\n",
            "Train loss at epoch 252 = 2.308194398880005\n",
            "Train loss at epoch 253 = 2.3123779296875\n",
            "Train loss at epoch 254 = 2.3007569313049316\n",
            "Train loss at epoch 255 = 2.330895185470581\n",
            "Train loss at epoch 256 = 2.2895994186401367\n",
            "Train loss at epoch 257 = 2.3254451751708984\n",
            "Train loss at epoch 258 = 2.3177943229675293\n",
            "Train loss at epoch 259 = 2.2795016765594482\n",
            "Train loss at epoch 260 = 2.281965732574463\n",
            "Train loss at epoch 261 = 2.3105199337005615\n",
            "Train loss at epoch 262 = 2.2892556190490723\n",
            "Train loss at epoch 263 = 2.2918667793273926\n",
            "Train loss at epoch 264 = 2.3112549781799316\n",
            "Train loss at epoch 265 = 2.2772915363311768\n",
            "Train loss at epoch 266 = 2.2549774646759033\n",
            "Train loss at epoch 267 = 2.278296709060669\n",
            "Train loss at epoch 268 = 2.269303798675537\n",
            "Train loss at epoch 269 = 2.2679848670959473\n",
            "Train loss at epoch 270 = 2.276902675628662\n",
            "Train loss at epoch 271 = 2.2415289878845215\n",
            "Train loss at epoch 272 = 2.2993953227996826\n",
            "Train loss at epoch 273 = 2.281856060028076\n",
            "Train loss at epoch 274 = 2.2476511001586914\n",
            "Train loss at epoch 275 = 2.264744997024536\n",
            "Train loss at epoch 276 = 2.2651612758636475\n",
            "Train loss at epoch 277 = 2.2803750038146973\n",
            "Train loss at epoch 278 = 2.2592387199401855\n",
            "Train loss at epoch 279 = 2.247377634048462\n",
            "Train loss at epoch 280 = 2.246309280395508\n",
            "Train loss at epoch 281 = 2.215418577194214\n",
            "Train loss at epoch 282 = 2.2195050716400146\n",
            "Train loss at epoch 283 = 2.2459540367126465\n",
            "Train loss at epoch 284 = 2.266624689102173\n",
            "Train loss at epoch 285 = 2.214171886444092\n",
            "Train loss at epoch 286 = 2.232090950012207\n",
            "Train loss at epoch 287 = 2.236140251159668\n",
            "Train loss at epoch 288 = 2.2527129650115967\n",
            "Train loss at epoch 289 = 2.206763982772827\n",
            "Train loss at epoch 290 = 2.203397750854492\n",
            "Train loss at epoch 291 = 2.2165486812591553\n",
            "Train loss at epoch 292 = 2.2151737213134766\n",
            "Train loss at epoch 293 = 2.2151169776916504\n",
            "Train loss at epoch 294 = 2.1935536861419678\n",
            "Train loss at epoch 295 = 2.2172508239746094\n",
            "Train loss at epoch 296 = 2.204962968826294\n",
            "Train loss at epoch 297 = 2.2091195583343506\n",
            "Train loss at epoch 298 = 2.2242531776428223\n",
            "Train loss at epoch 299 = 2.190180778503418\n",
            "Train loss at epoch 300 = 2.2186315059661865\n",
            "Train loss at epoch 301 = 2.1992993354797363\n",
            "Train loss at epoch 302 = 2.1970956325531006\n",
            "Train loss at epoch 303 = 2.174666166305542\n",
            "Train loss at epoch 304 = 2.1909968852996826\n",
            "Train loss at epoch 305 = 2.196035623550415\n",
            "Train loss at epoch 306 = 2.1746087074279785\n",
            "Train loss at epoch 307 = 2.184748888015747\n",
            "Train loss at epoch 308 = 2.181126356124878\n",
            "Train loss at epoch 309 = 2.1722960472106934\n",
            "Train loss at epoch 310 = 2.173844575881958\n",
            "Train loss at epoch 311 = 2.197680711746216\n",
            "Train loss at epoch 312 = 2.152804374694824\n",
            "Train loss at epoch 313 = 2.1723387241363525\n",
            "Train loss at epoch 314 = 2.1758573055267334\n",
            "Train loss at epoch 315 = 2.180812120437622\n",
            "Train loss at epoch 316 = 2.1643245220184326\n",
            "Train loss at epoch 317 = 2.17866849899292\n",
            "Train loss at epoch 318 = 2.1588714122772217\n",
            "Train loss at epoch 319 = 2.1726629734039307\n",
            "Train loss at epoch 320 = 2.172978162765503\n",
            "Train loss at epoch 321 = 2.1568796634674072\n",
            "Train loss at epoch 322 = 2.1619224548339844\n",
            "Train loss at epoch 323 = 2.1739721298217773\n",
            "Train loss at epoch 324 = 2.151073455810547\n",
            "Train loss at epoch 325 = 2.1645348072052\n",
            "Train loss at epoch 326 = 2.1303980350494385\n",
            "Train loss at epoch 327 = 2.1587629318237305\n",
            "Train loss at epoch 328 = 2.14446759223938\n",
            "Train loss at epoch 329 = 2.1583149433135986\n",
            "Train loss at epoch 330 = 2.1526458263397217\n",
            "Train loss at epoch 331 = 2.1566669940948486\n",
            "Train loss at epoch 332 = 2.1371610164642334\n",
            "Train loss at epoch 333 = 2.1270956993103027\n",
            "Train loss at epoch 334 = 2.1247856616973877\n",
            "Train loss at epoch 335 = 2.1210718154907227\n",
            "Train loss at epoch 336 = 2.1224284172058105\n",
            "Train loss at epoch 337 = 2.144202470779419\n",
            "Train loss at epoch 338 = 2.1075549125671387\n",
            "Train loss at epoch 339 = 2.1473312377929688\n",
            "Train loss at epoch 340 = 2.1385929584503174\n",
            "Train loss at epoch 341 = 2.1332030296325684\n",
            "Train loss at epoch 342 = 2.1263160705566406\n",
            "Train loss at epoch 343 = 2.10583758354187\n",
            "Train loss at epoch 344 = 2.1175522804260254\n",
            "Train loss at epoch 345 = 2.121126413345337\n",
            "Train loss at epoch 346 = 2.1268069744110107\n",
            "Train loss at epoch 347 = 2.0972626209259033\n",
            "Train loss at epoch 348 = 2.0868189334869385\n",
            "Train loss at epoch 349 = 2.105557441711426\n",
            "Train loss at epoch 350 = 2.1062088012695312\n",
            "Train loss at epoch 351 = 2.096122980117798\n",
            "Train loss at epoch 352 = 2.119220495223999\n",
            "Train loss at epoch 353 = 2.0989267826080322\n",
            "Train loss at epoch 354 = 2.1067488193511963\n",
            "Train loss at epoch 355 = 2.097262382507324\n",
            "Train loss at epoch 356 = 2.0817151069641113\n",
            "Train loss at epoch 357 = 2.0982651710510254\n",
            "Train loss at epoch 358 = 2.0731823444366455\n",
            "Train loss at epoch 359 = 2.123103618621826\n",
            "Train loss at epoch 360 = 2.091214656829834\n",
            "Train loss at epoch 361 = 2.088306188583374\n",
            "Train loss at epoch 362 = 2.0819501876831055\n",
            "Train loss at epoch 363 = 2.09980845451355\n",
            "Train loss at epoch 364 = 2.085693120956421\n",
            "Train loss at epoch 365 = 2.0728983879089355\n",
            "Train loss at epoch 366 = 2.0993082523345947\n",
            "Train loss at epoch 367 = 2.081416368484497\n",
            "Train loss at epoch 368 = 2.100790500640869\n",
            "Train loss at epoch 369 = 2.066910982131958\n",
            "Train loss at epoch 370 = 2.070460319519043\n",
            "Train loss at epoch 371 = 2.0660316944122314\n",
            "Train loss at epoch 372 = 2.090156316757202\n",
            "Train loss at epoch 373 = 2.0861237049102783\n",
            "Train loss at epoch 374 = 2.0657782554626465\n",
            "Train loss at epoch 375 = 2.0808565616607666\n",
            "Train loss at epoch 376 = 2.0768797397613525\n",
            "Train loss at epoch 377 = 2.0569207668304443\n",
            "Train loss at epoch 378 = 2.0528950691223145\n",
            "Train loss at epoch 379 = 2.0732882022857666\n",
            "Train loss at epoch 380 = 2.0798048973083496\n",
            "Train loss at epoch 381 = 2.06792950630188\n",
            "Train loss at epoch 382 = 2.0507659912109375\n",
            "Train loss at epoch 383 = 2.0355420112609863\n",
            "Train loss at epoch 384 = 2.0665855407714844\n",
            "Train loss at epoch 385 = 2.0341475009918213\n",
            "Train loss at epoch 386 = 2.046499729156494\n",
            "Train loss at epoch 387 = 2.0510029792785645\n",
            "Train loss at epoch 388 = 2.067103385925293\n",
            "Train loss at epoch 389 = 2.046718120574951\n",
            "Train loss at epoch 390 = 2.0735223293304443\n",
            "Train loss at epoch 391 = 2.062549114227295\n",
            "Train loss at epoch 392 = 2.029327869415283\n",
            "Train loss at epoch 393 = 2.0257349014282227\n",
            "Train loss at epoch 394 = 2.021953582763672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id = torch.zeros((1,1), dtype= torch.long)\n",
        "print(decode(m.generate(id,max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "O62ROfpT7nUb",
        "outputId": "e5d88b9d-73a5-4db4-e74f-a5ba3de851ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e9832d0349dd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    }
  ]
}