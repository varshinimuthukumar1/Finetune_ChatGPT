{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx6WegpLi_LS",
        "outputId": "7a3bcaea-3e83-4d4c-dd4d-a54cadf5fdf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-24 16:50:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-07-24 16:50:31 (23.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBQozQ_FuEtK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "uWdbL-JijQdX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters:\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7cm_tkpjb1P",
        "outputId": "fcf47a19-b18a-4b75-a00c-96dc1c9b86b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0--OGtYjjmPe",
        "outputId": "8909eddb-56af-487b-e2a5-4cf2ec9c2cab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhmWzi5jjrMC",
        "outputId": "a9cb0ad8-4c40-4826-ebaa-7dc0dd538a2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenize --- convert raw text to some sequence of integers according to some vocabulary\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s:[stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hi there\"))\n",
        "print(decode(encode(\"hi there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fITw6-xLkCr0",
        "outputId": "6f0079e1-9b76-4c2e-a8d9-ae1fc2dd1dbc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 1, 58, 46, 43, 56, 43]\n",
            "hi there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UetzwQk2hOHB",
        "outputId": "daaa10d6-1ff7-4856-e95a-b153d3fa3e5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "loI-RkdahhQy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjNQfAU0h2SN",
        "outputId": "cbafcf3f-29fd-42bc-f090-ee43e6685bf7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is:{target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jwAK2Wwh9Kr",
        "outputId": "e13cc785-2159-4a35-fdca-dd3fefa15793"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is:47\n",
            "when input is tensor([18, 47]) the target is:56\n",
            "when input is tensor([18, 47, 56]) the target is:57\n",
            "when input is tensor([18, 47, 56, 57]) the target is:58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is:1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is:15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is:47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is:58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size=4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split==\"train\" else val_data\n",
        "  ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----------')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X42kD-28im6V",
        "outputId": "656d2a10-4683-4f29-d374-5af7cb6563d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----------\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Bigram model as a baseline"
      ],
      "metadata": {
        "id": "irTHVD5PvDOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Simple baseline model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 8 # what is the maximum context length for the predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate= 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is the (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits,loss = self(idx)\n",
        "      # forcus only on the last time step\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      # applend sampled index to the running sequene\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "out, loss = m(xb,yb)\n",
        "print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "id = torch.zeros((1,1), dtype= torch.long)\n",
        "print(decode(m.generate(id,max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "### Initial loss will be -ln(1/65) as we have 65 possible vocabulary elements\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "judppe4wnvzU",
        "outputId": "c3ce36e0-38f6-469e-9947-24b76c4fb039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(m.parameters(), lr= 1e-3)"
      ],
      "metadata": {
        "id": "haxcTP9kpmXO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "220a8272-337c-4c39-a4f0-4e454a1d6e51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c7e1aab3d4f3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for steps in range(10000):\n",
        "  xb,yb = get_batch('train')\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "O_PSVwSssw7h",
        "outputId": "bfc629cc-9c5c-487c-f808-e6ec4703fd0e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4ead05908c63>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype= torch.long), max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI1vmswctZY-",
        "outputId": "68568030-4a36-4154-d16e-87db1984b109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tom ndiske\n",
            "tukistlinas:\n",
            "\n",
            "Orat lyowe wiclin:\n",
            "ALLars t th cinod as!\n",
            "Thevedevead;AEJUCaroftt,\n",
            "ADerde hy s tr;\n",
            "GSt cicoutrncolond p f IAwathaner:\n",
            "ONat they t fo be gs blle weamyoure itintet in'sut loomalll ink!\n",
            "F s sur,\n",
            "Thaveine thayouditelfrenevecyowe tes, hile\n",
            "Fas d,\n",
            "TID:\n",
            "\n",
            "\n",
            "OME sheve ind f bevk, nes k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention"
      ],
      "metadata": {
        "id": "4zAyg9K-vKSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1iX1HROvNAV",
        "outputId": "1e193ae1-af8c-4563-b02c-44eade7eea0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "## Hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256 # what is the maximum context length for the predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 500\n",
        "learning_rate= 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_layer =6\n",
        "n_head = 6\n",
        "dropout = 0.2\n",
        "\n",
        "# ---------------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) # (B,T,C)\n",
        "    q = self.query(x) # (B,T,C)\n",
        "    v = self.value(x) # (B,T,C)\n",
        "\n",
        "    # compute self-attention or affinities among the key and query\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1)  #(B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) #(B,T,C)\n",
        "    out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    # n_embd : embedding dimension, n_head: the number of heads we'd like\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x)) # x + self.sa is the residual skip connections\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device= device)) # (T,C)\n",
        "    x = tok_emb + pos_emb # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # (B,T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is the (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits,loss = self(idx_cond)\n",
        "      # forcus only on the last time step\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      # applend sampled index to the running sequene\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel()\n",
        "out, loss = m(xb,yb)\n",
        "print(out.shape)\n",
        "print(loss)\n",
        "\n",
        "id = torch.zeros((1,1), dtype= torch.long)\n",
        "print(decode(m.generate(id,max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "### Initial loss will be -ln(1/65) as we have 65 possible vocabulary elements\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHagMBuauFuv",
        "outputId": "4e816da6-220f-443f-9efb-083b8fc4f414"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(5.1208, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "y3,MS;KWohAJDg Z &Yo&LrnQjAEBolP'EAexQfm3GYfrn$Hk&iPomcXcfW iKPzM\n",
            "yUrupQl.Tpl..Wdd-KhIi.G arxcpAf dz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr= learning_rate)\n",
        "for steps in range(max_iters):\n",
        "  xb,yb = get_batch('train')\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Train loss at epoch {steps} = {loss.item()}\")\n",
        "  if steps%eval_interval == 0:\n",
        "    xv,yv = get_batch(\"val\")\n",
        "    logits, loss = m(xv,yv)\n",
        "    print(f\"val loss = {loss.item()}\" )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_voZ9KYEy4-j",
        "outputId": "4f131648-f6bf-4385-8d39-fa0bbd356b11"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss at epoch 0 = 4.526788234710693\n",
            "val loss = 3.90280818939209\n",
            "Train loss at epoch 1 = 3.8721046447753906\n",
            "Train loss at epoch 2 = 3.943657875061035\n",
            "Train loss at epoch 3 = 3.748080253601074\n",
            "Train loss at epoch 4 = 3.496762275695801\n",
            "Train loss at epoch 5 = 3.2048966884613037\n",
            "Train loss at epoch 6 = 3.2369601726531982\n",
            "Train loss at epoch 7 = 3.1424763202667236\n",
            "Train loss at epoch 8 = 3.053337335586548\n",
            "Train loss at epoch 9 = 3.0583434104919434\n",
            "Train loss at epoch 10 = 2.983870506286621\n",
            "Train loss at epoch 11 = 2.9792001247406006\n",
            "Train loss at epoch 12 = 2.9429593086242676\n",
            "Train loss at epoch 13 = 2.882878303527832\n",
            "Train loss at epoch 14 = 2.8609464168548584\n",
            "Train loss at epoch 15 = 2.877351760864258\n",
            "Train loss at epoch 16 = 2.818812847137451\n",
            "Train loss at epoch 17 = 2.823803424835205\n",
            "Train loss at epoch 18 = 2.7864596843719482\n",
            "Train loss at epoch 19 = 2.767024517059326\n",
            "Train loss at epoch 20 = 2.742643356323242\n",
            "Train loss at epoch 21 = 2.7793896198272705\n",
            "Train loss at epoch 22 = 2.741788625717163\n",
            "Train loss at epoch 23 = 2.7359354496002197\n",
            "Train loss at epoch 24 = 2.7234373092651367\n",
            "Train loss at epoch 25 = 2.661454200744629\n",
            "Train loss at epoch 26 = 2.6726834774017334\n",
            "Train loss at epoch 27 = 2.675889730453491\n",
            "Train loss at epoch 28 = 2.663159132003784\n",
            "Train loss at epoch 29 = 2.6547598838806152\n",
            "Train loss at epoch 30 = 2.639833688735962\n",
            "Train loss at epoch 31 = 2.6393637657165527\n",
            "Train loss at epoch 32 = 2.630910873413086\n",
            "Train loss at epoch 33 = 2.6474688053131104\n",
            "Train loss at epoch 34 = 2.626903772354126\n",
            "Train loss at epoch 35 = 2.6332943439483643\n",
            "Train loss at epoch 36 = 2.6175360679626465\n",
            "Train loss at epoch 37 = 2.620633602142334\n",
            "Train loss at epoch 38 = 2.5767977237701416\n",
            "Train loss at epoch 39 = 2.6085643768310547\n",
            "Train loss at epoch 40 = 2.585082769393921\n",
            "Train loss at epoch 41 = 2.596743583679199\n",
            "Train loss at epoch 42 = 2.5973849296569824\n",
            "Train loss at epoch 43 = 2.6097660064697266\n",
            "Train loss at epoch 44 = 2.5857620239257812\n",
            "Train loss at epoch 45 = 2.586200475692749\n",
            "Train loss at epoch 46 = 2.584059953689575\n",
            "Train loss at epoch 47 = 2.558525800704956\n",
            "Train loss at epoch 48 = 2.5911855697631836\n",
            "Train loss at epoch 49 = 2.569991111755371\n",
            "Train loss at epoch 50 = 2.580319881439209\n",
            "Train loss at epoch 51 = 2.545140266418457\n",
            "Train loss at epoch 52 = 2.5761048793792725\n",
            "Train loss at epoch 53 = 2.5844876766204834\n",
            "Train loss at epoch 54 = 2.5494565963745117\n",
            "Train loss at epoch 55 = 2.5619702339172363\n",
            "Train loss at epoch 56 = 2.5465784072875977\n",
            "Train loss at epoch 57 = 2.550117254257202\n",
            "Train loss at epoch 58 = 2.557454824447632\n",
            "Train loss at epoch 59 = 2.5444579124450684\n",
            "Train loss at epoch 60 = 2.5612387657165527\n",
            "Train loss at epoch 61 = 2.551600694656372\n",
            "Train loss at epoch 62 = 2.540555477142334\n",
            "Train loss at epoch 63 = 2.5390031337738037\n",
            "Train loss at epoch 64 = 2.536409378051758\n",
            "Train loss at epoch 65 = 2.560516119003296\n",
            "Train loss at epoch 66 = 2.5188069343566895\n",
            "Train loss at epoch 67 = 2.5150043964385986\n",
            "Train loss at epoch 68 = 2.540975332260132\n",
            "Train loss at epoch 69 = 2.5414633750915527\n",
            "Train loss at epoch 70 = 2.5248517990112305\n",
            "Train loss at epoch 71 = 2.5403294563293457\n",
            "Train loss at epoch 72 = 2.5552992820739746\n",
            "Train loss at epoch 73 = 2.523063898086548\n",
            "Train loss at epoch 74 = 2.535938262939453\n",
            "Train loss at epoch 75 = 2.5170700550079346\n",
            "Train loss at epoch 76 = 2.523383378982544\n",
            "Train loss at epoch 77 = 2.519375801086426\n",
            "Train loss at epoch 78 = 2.5356786251068115\n",
            "Train loss at epoch 79 = 2.5291178226470947\n",
            "Train loss at epoch 80 = 2.538313627243042\n",
            "Train loss at epoch 81 = 2.4970104694366455\n",
            "Train loss at epoch 82 = 2.5134241580963135\n",
            "Train loss at epoch 83 = 2.5254111289978027\n",
            "Train loss at epoch 84 = 2.539280414581299\n",
            "Train loss at epoch 85 = 2.515399932861328\n",
            "Train loss at epoch 86 = 2.5236124992370605\n",
            "Train loss at epoch 87 = 2.5034914016723633\n",
            "Train loss at epoch 88 = 2.5137603282928467\n",
            "Train loss at epoch 89 = 2.50642466545105\n",
            "Train loss at epoch 90 = 2.5276951789855957\n",
            "Train loss at epoch 91 = 2.515993118286133\n",
            "Train loss at epoch 92 = 2.5092012882232666\n",
            "Train loss at epoch 93 = 2.5221588611602783\n",
            "Train loss at epoch 94 = 2.5175771713256836\n",
            "Train loss at epoch 95 = 2.4995269775390625\n",
            "Train loss at epoch 96 = 2.5407447814941406\n",
            "Train loss at epoch 97 = 2.50001859664917\n",
            "Train loss at epoch 98 = 2.50260591506958\n",
            "Train loss at epoch 99 = 2.5082900524139404\n",
            "Train loss at epoch 100 = 2.491504669189453\n",
            "Train loss at epoch 101 = 2.4996402263641357\n",
            "Train loss at epoch 102 = 2.5223231315612793\n",
            "Train loss at epoch 103 = 2.4935684204101562\n",
            "Train loss at epoch 104 = 2.5167462825775146\n",
            "Train loss at epoch 105 = 2.4948418140411377\n",
            "Train loss at epoch 106 = 2.500215530395508\n",
            "Train loss at epoch 107 = 2.5033011436462402\n",
            "Train loss at epoch 108 = 2.501988410949707\n",
            "Train loss at epoch 109 = 2.502920150756836\n",
            "Train loss at epoch 110 = 2.4954535961151123\n",
            "Train loss at epoch 111 = 2.491189956665039\n",
            "Train loss at epoch 112 = 2.4863178730010986\n",
            "Train loss at epoch 113 = 2.4616100788116455\n",
            "Train loss at epoch 114 = 2.5153801441192627\n",
            "Train loss at epoch 115 = 2.4878201484680176\n",
            "Train loss at epoch 116 = 2.4809226989746094\n",
            "Train loss at epoch 117 = 2.5014078617095947\n",
            "Train loss at epoch 118 = 2.469440460205078\n",
            "Train loss at epoch 119 = 2.5022592544555664\n",
            "Train loss at epoch 120 = 2.5034072399139404\n",
            "Train loss at epoch 121 = 2.5027711391448975\n",
            "Train loss at epoch 122 = 2.4903244972229004\n",
            "Train loss at epoch 123 = 2.5030720233917236\n",
            "Train loss at epoch 124 = 2.4948086738586426\n",
            "Train loss at epoch 125 = 2.479509115219116\n",
            "Train loss at epoch 126 = 2.461035966873169\n",
            "Train loss at epoch 127 = 2.4846436977386475\n",
            "Train loss at epoch 128 = 2.5047593116760254\n",
            "Train loss at epoch 129 = 2.5043163299560547\n",
            "Train loss at epoch 130 = 2.495509386062622\n",
            "Train loss at epoch 131 = 2.49657940864563\n",
            "Train loss at epoch 132 = 2.4964373111724854\n",
            "Train loss at epoch 133 = 2.4975152015686035\n",
            "Train loss at epoch 134 = 2.4824612140655518\n",
            "Train loss at epoch 135 = 2.496056318283081\n",
            "Train loss at epoch 136 = 2.4745664596557617\n",
            "Train loss at epoch 137 = 2.475792646408081\n",
            "Train loss at epoch 138 = 2.474609136581421\n",
            "Train loss at epoch 139 = 2.484118938446045\n",
            "Train loss at epoch 140 = 2.491791009902954\n",
            "Train loss at epoch 141 = 2.4737327098846436\n",
            "Train loss at epoch 142 = 2.4954144954681396\n",
            "Train loss at epoch 143 = 2.4689929485321045\n",
            "Train loss at epoch 144 = 2.5098564624786377\n",
            "Train loss at epoch 145 = 2.462329149246216\n",
            "Train loss at epoch 146 = 2.477365255355835\n",
            "Train loss at epoch 147 = 2.488312005996704\n",
            "Train loss at epoch 148 = 2.484661102294922\n",
            "Train loss at epoch 149 = 2.4880282878875732\n",
            "Train loss at epoch 150 = 2.4897663593292236\n",
            "Train loss at epoch 151 = 2.4891610145568848\n",
            "Train loss at epoch 152 = 2.477313280105591\n",
            "Train loss at epoch 153 = 2.4455888271331787\n",
            "Train loss at epoch 154 = 2.4716498851776123\n",
            "Train loss at epoch 155 = 2.4544594287872314\n",
            "Train loss at epoch 156 = 2.4839141368865967\n",
            "Train loss at epoch 157 = 2.455294609069824\n",
            "Train loss at epoch 158 = 2.472097158432007\n",
            "Train loss at epoch 159 = 2.4754738807678223\n",
            "Train loss at epoch 160 = 2.483163833618164\n",
            "Train loss at epoch 161 = 2.481043815612793\n",
            "Train loss at epoch 162 = 2.4447834491729736\n",
            "Train loss at epoch 163 = 2.4660847187042236\n",
            "Train loss at epoch 164 = 2.4699723720550537\n",
            "Train loss at epoch 165 = 2.467998504638672\n",
            "Train loss at epoch 166 = 2.4523723125457764\n",
            "Train loss at epoch 167 = 2.482908248901367\n",
            "Train loss at epoch 168 = 2.4599549770355225\n",
            "Train loss at epoch 169 = 2.4641640186309814\n",
            "Train loss at epoch 170 = 2.4540441036224365\n",
            "Train loss at epoch 171 = 2.461272954940796\n",
            "Train loss at epoch 172 = 2.468597412109375\n",
            "Train loss at epoch 173 = 2.441685676574707\n",
            "Train loss at epoch 174 = 2.449110746383667\n",
            "Train loss at epoch 175 = 2.465585470199585\n",
            "Train loss at epoch 176 = 2.44469952583313\n",
            "Train loss at epoch 177 = 2.4537899494171143\n",
            "Train loss at epoch 178 = 2.4520275592803955\n",
            "Train loss at epoch 179 = 2.4497032165527344\n",
            "Train loss at epoch 180 = 2.442732810974121\n",
            "Train loss at epoch 181 = 2.4339122772216797\n",
            "Train loss at epoch 182 = 2.453582525253296\n",
            "Train loss at epoch 183 = 2.471203327178955\n",
            "Train loss at epoch 184 = 2.471369504928589\n",
            "Train loss at epoch 185 = 2.452699661254883\n",
            "Train loss at epoch 186 = 2.4499380588531494\n",
            "Train loss at epoch 187 = 2.45973539352417\n",
            "Train loss at epoch 188 = 2.442481279373169\n",
            "Train loss at epoch 189 = 2.4560422897338867\n",
            "Train loss at epoch 190 = 2.446042776107788\n",
            "Train loss at epoch 191 = 2.4323062896728516\n",
            "Train loss at epoch 192 = 2.4478981494903564\n",
            "Train loss at epoch 193 = 2.446810483932495\n",
            "Train loss at epoch 194 = 2.4434149265289307\n",
            "Train loss at epoch 195 = 2.4479339122772217\n",
            "Train loss at epoch 196 = 2.4364912509918213\n",
            "Train loss at epoch 197 = 2.441166400909424\n",
            "Train loss at epoch 198 = 2.4343888759613037\n",
            "Train loss at epoch 199 = 2.4309370517730713\n",
            "Train loss at epoch 200 = 2.431394338607788\n",
            "Train loss at epoch 201 = 2.4147095680236816\n",
            "Train loss at epoch 202 = 2.4328291416168213\n",
            "Train loss at epoch 203 = 2.4360885620117188\n",
            "Train loss at epoch 204 = 2.442579746246338\n",
            "Train loss at epoch 205 = 2.4277093410491943\n",
            "Train loss at epoch 206 = 2.430264711380005\n",
            "Train loss at epoch 207 = 2.4257166385650635\n",
            "Train loss at epoch 208 = 2.403761625289917\n",
            "Train loss at epoch 209 = 2.397799015045166\n",
            "Train loss at epoch 210 = 2.42516827583313\n",
            "Train loss at epoch 211 = 2.421576499938965\n",
            "Train loss at epoch 212 = 2.4162697792053223\n",
            "Train loss at epoch 213 = 2.3956542015075684\n",
            "Train loss at epoch 214 = 2.4015114307403564\n",
            "Train loss at epoch 215 = 2.4241676330566406\n",
            "Train loss at epoch 216 = 2.4117119312286377\n",
            "Train loss at epoch 217 = 2.4113810062408447\n",
            "Train loss at epoch 218 = 2.379110336303711\n",
            "Train loss at epoch 219 = 2.4087321758270264\n",
            "Train loss at epoch 220 = 2.399623155593872\n",
            "Train loss at epoch 221 = 2.389253854751587\n",
            "Train loss at epoch 222 = 2.385883331298828\n",
            "Train loss at epoch 223 = 2.400765895843506\n",
            "Train loss at epoch 224 = 2.393848419189453\n",
            "Train loss at epoch 225 = 2.4000000953674316\n",
            "Train loss at epoch 226 = 2.3801863193511963\n",
            "Train loss at epoch 227 = 2.379347085952759\n",
            "Train loss at epoch 228 = 2.3707118034362793\n",
            "Train loss at epoch 229 = 2.3788321018218994\n",
            "Train loss at epoch 230 = 2.371354818344116\n",
            "Train loss at epoch 231 = 2.376768112182617\n",
            "Train loss at epoch 232 = 2.3613882064819336\n",
            "Train loss at epoch 233 = 2.3765738010406494\n",
            "Train loss at epoch 234 = 2.3756957054138184\n",
            "Train loss at epoch 235 = 2.3440299034118652\n",
            "Train loss at epoch 236 = 2.3767752647399902\n",
            "Train loss at epoch 237 = 2.3602805137634277\n",
            "Train loss at epoch 238 = 2.338364601135254\n",
            "Train loss at epoch 239 = 2.3382346630096436\n",
            "Train loss at epoch 240 = 2.3437228202819824\n",
            "Train loss at epoch 241 = 2.348155975341797\n",
            "Train loss at epoch 242 = 2.3286173343658447\n",
            "Train loss at epoch 243 = 2.338743209838867\n",
            "Train loss at epoch 244 = 2.3261704444885254\n",
            "Train loss at epoch 245 = 2.3282344341278076\n",
            "Train loss at epoch 246 = 2.3274269104003906\n",
            "Train loss at epoch 247 = 2.3307628631591797\n",
            "Train loss at epoch 248 = 2.3247621059417725\n",
            "Train loss at epoch 249 = 2.346813917160034\n",
            "Train loss at epoch 250 = 2.311215400695801\n",
            "Train loss at epoch 251 = 2.3052425384521484\n",
            "Train loss at epoch 252 = 2.308194398880005\n",
            "Train loss at epoch 253 = 2.3123779296875\n",
            "Train loss at epoch 254 = 2.3007569313049316\n",
            "Train loss at epoch 255 = 2.330895185470581\n",
            "Train loss at epoch 256 = 2.2895994186401367\n",
            "Train loss at epoch 257 = 2.3254451751708984\n",
            "Train loss at epoch 258 = 2.3177943229675293\n",
            "Train loss at epoch 259 = 2.2795016765594482\n",
            "Train loss at epoch 260 = 2.281965732574463\n",
            "Train loss at epoch 261 = 2.3105199337005615\n",
            "Train loss at epoch 262 = 2.2892556190490723\n",
            "Train loss at epoch 263 = 2.2918667793273926\n",
            "Train loss at epoch 264 = 2.3112549781799316\n",
            "Train loss at epoch 265 = 2.2772915363311768\n",
            "Train loss at epoch 266 = 2.2549774646759033\n",
            "Train loss at epoch 267 = 2.278296709060669\n",
            "Train loss at epoch 268 = 2.269303798675537\n",
            "Train loss at epoch 269 = 2.2679848670959473\n",
            "Train loss at epoch 270 = 2.276902675628662\n",
            "Train loss at epoch 271 = 2.2415289878845215\n",
            "Train loss at epoch 272 = 2.2993953227996826\n",
            "Train loss at epoch 273 = 2.281856060028076\n",
            "Train loss at epoch 274 = 2.2476511001586914\n",
            "Train loss at epoch 275 = 2.264744997024536\n",
            "Train loss at epoch 276 = 2.2651612758636475\n",
            "Train loss at epoch 277 = 2.2803750038146973\n",
            "Train loss at epoch 278 = 2.2592387199401855\n",
            "Train loss at epoch 279 = 2.247377634048462\n",
            "Train loss at epoch 280 = 2.246309280395508\n",
            "Train loss at epoch 281 = 2.215418577194214\n",
            "Train loss at epoch 282 = 2.2195050716400146\n",
            "Train loss at epoch 283 = 2.2459540367126465\n",
            "Train loss at epoch 284 = 2.266624689102173\n",
            "Train loss at epoch 285 = 2.214171886444092\n",
            "Train loss at epoch 286 = 2.232090950012207\n",
            "Train loss at epoch 287 = 2.236140251159668\n",
            "Train loss at epoch 288 = 2.2527129650115967\n",
            "Train loss at epoch 289 = 2.206763982772827\n",
            "Train loss at epoch 290 = 2.203397750854492\n",
            "Train loss at epoch 291 = 2.2165486812591553\n",
            "Train loss at epoch 292 = 2.2151737213134766\n",
            "Train loss at epoch 293 = 2.2151169776916504\n",
            "Train loss at epoch 294 = 2.1935536861419678\n",
            "Train loss at epoch 295 = 2.2172508239746094\n",
            "Train loss at epoch 296 = 2.204962968826294\n",
            "Train loss at epoch 297 = 2.2091195583343506\n",
            "Train loss at epoch 298 = 2.2242531776428223\n",
            "Train loss at epoch 299 = 2.190180778503418\n",
            "Train loss at epoch 300 = 2.2186315059661865\n",
            "Train loss at epoch 301 = 2.1992993354797363\n",
            "Train loss at epoch 302 = 2.1970956325531006\n",
            "Train loss at epoch 303 = 2.174666166305542\n",
            "Train loss at epoch 304 = 2.1909968852996826\n",
            "Train loss at epoch 305 = 2.196035623550415\n",
            "Train loss at epoch 306 = 2.1746087074279785\n",
            "Train loss at epoch 307 = 2.184748888015747\n",
            "Train loss at epoch 308 = 2.181126356124878\n",
            "Train loss at epoch 309 = 2.1722960472106934\n",
            "Train loss at epoch 310 = 2.173844575881958\n",
            "Train loss at epoch 311 = 2.197680711746216\n",
            "Train loss at epoch 312 = 2.152804374694824\n",
            "Train loss at epoch 313 = 2.1723387241363525\n",
            "Train loss at epoch 314 = 2.1758573055267334\n",
            "Train loss at epoch 315 = 2.180812120437622\n",
            "Train loss at epoch 316 = 2.1643245220184326\n",
            "Train loss at epoch 317 = 2.17866849899292\n",
            "Train loss at epoch 318 = 2.1588714122772217\n",
            "Train loss at epoch 319 = 2.1726629734039307\n",
            "Train loss at epoch 320 = 2.172978162765503\n",
            "Train loss at epoch 321 = 2.1568796634674072\n",
            "Train loss at epoch 322 = 2.1619224548339844\n",
            "Train loss at epoch 323 = 2.1739721298217773\n",
            "Train loss at epoch 324 = 2.151073455810547\n",
            "Train loss at epoch 325 = 2.1645348072052\n",
            "Train loss at epoch 326 = 2.1303980350494385\n",
            "Train loss at epoch 327 = 2.1587629318237305\n",
            "Train loss at epoch 328 = 2.14446759223938\n",
            "Train loss at epoch 329 = 2.1583149433135986\n",
            "Train loss at epoch 330 = 2.1526458263397217\n",
            "Train loss at epoch 331 = 2.1566669940948486\n",
            "Train loss at epoch 332 = 2.1371610164642334\n",
            "Train loss at epoch 333 = 2.1270956993103027\n",
            "Train loss at epoch 334 = 2.1247856616973877\n",
            "Train loss at epoch 335 = 2.1210718154907227\n",
            "Train loss at epoch 336 = 2.1224284172058105\n",
            "Train loss at epoch 337 = 2.144202470779419\n",
            "Train loss at epoch 338 = 2.1075549125671387\n",
            "Train loss at epoch 339 = 2.1473312377929688\n",
            "Train loss at epoch 340 = 2.1385929584503174\n",
            "Train loss at epoch 341 = 2.1332030296325684\n",
            "Train loss at epoch 342 = 2.1263160705566406\n",
            "Train loss at epoch 343 = 2.10583758354187\n",
            "Train loss at epoch 344 = 2.1175522804260254\n",
            "Train loss at epoch 345 = 2.121126413345337\n",
            "Train loss at epoch 346 = 2.1268069744110107\n",
            "Train loss at epoch 347 = 2.0972626209259033\n",
            "Train loss at epoch 348 = 2.0868189334869385\n",
            "Train loss at epoch 349 = 2.105557441711426\n",
            "Train loss at epoch 350 = 2.1062088012695312\n",
            "Train loss at epoch 351 = 2.096122980117798\n",
            "Train loss at epoch 352 = 2.119220495223999\n",
            "Train loss at epoch 353 = 2.0989267826080322\n",
            "Train loss at epoch 354 = 2.1067488193511963\n",
            "Train loss at epoch 355 = 2.097262382507324\n",
            "Train loss at epoch 356 = 2.0817151069641113\n",
            "Train loss at epoch 357 = 2.0982651710510254\n",
            "Train loss at epoch 358 = 2.0731823444366455\n",
            "Train loss at epoch 359 = 2.123103618621826\n",
            "Train loss at epoch 360 = 2.091214656829834\n",
            "Train loss at epoch 361 = 2.088306188583374\n",
            "Train loss at epoch 362 = 2.0819501876831055\n",
            "Train loss at epoch 363 = 2.09980845451355\n",
            "Train loss at epoch 364 = 2.085693120956421\n",
            "Train loss at epoch 365 = 2.0728983879089355\n",
            "Train loss at epoch 366 = 2.0993082523345947\n",
            "Train loss at epoch 367 = 2.081416368484497\n",
            "Train loss at epoch 368 = 2.100790500640869\n",
            "Train loss at epoch 369 = 2.066910982131958\n",
            "Train loss at epoch 370 = 2.070460319519043\n",
            "Train loss at epoch 371 = 2.0660316944122314\n",
            "Train loss at epoch 372 = 2.090156316757202\n",
            "Train loss at epoch 373 = 2.0861237049102783\n",
            "Train loss at epoch 374 = 2.0657782554626465\n",
            "Train loss at epoch 375 = 2.0808565616607666\n",
            "Train loss at epoch 376 = 2.0768797397613525\n",
            "Train loss at epoch 377 = 2.0569207668304443\n",
            "Train loss at epoch 378 = 2.0528950691223145\n",
            "Train loss at epoch 379 = 2.0732882022857666\n",
            "Train loss at epoch 380 = 2.0798048973083496\n",
            "Train loss at epoch 381 = 2.06792950630188\n",
            "Train loss at epoch 382 = 2.0507659912109375\n",
            "Train loss at epoch 383 = 2.0355420112609863\n",
            "Train loss at epoch 384 = 2.0665855407714844\n",
            "Train loss at epoch 385 = 2.0341475009918213\n",
            "Train loss at epoch 386 = 2.046499729156494\n",
            "Train loss at epoch 387 = 2.0510029792785645\n",
            "Train loss at epoch 388 = 2.067103385925293\n",
            "Train loss at epoch 389 = 2.046718120574951\n",
            "Train loss at epoch 390 = 2.0735223293304443\n",
            "Train loss at epoch 391 = 2.062549114227295\n",
            "Train loss at epoch 392 = 2.029327869415283\n",
            "Train loss at epoch 393 = 2.0257349014282227\n",
            "Train loss at epoch 394 = 2.021953582763672\n",
            "Train loss at epoch 395 = 2.0231659412384033\n",
            "Train loss at epoch 396 = 2.062920093536377\n",
            "Train loss at epoch 397 = 2.0069282054901123\n",
            "Train loss at epoch 398 = 2.0152781009674072\n",
            "Train loss at epoch 399 = 2.0477640628814697\n",
            "Train loss at epoch 400 = 2.0255699157714844\n",
            "Train loss at epoch 401 = 2.0434844493865967\n",
            "Train loss at epoch 402 = 2.0451667308807373\n",
            "Train loss at epoch 403 = 2.050577402114868\n",
            "Train loss at epoch 404 = 2.021134853363037\n",
            "Train loss at epoch 405 = 2.030667543411255\n",
            "Train loss at epoch 406 = 2.0288820266723633\n",
            "Train loss at epoch 407 = 1.9955133199691772\n",
            "Train loss at epoch 408 = 2.0329208374023438\n",
            "Train loss at epoch 409 = 2.004305124282837\n",
            "Train loss at epoch 410 = 2.0236470699310303\n",
            "Train loss at epoch 411 = 2.048072099685669\n",
            "Train loss at epoch 412 = 2.0015029907226562\n",
            "Train loss at epoch 413 = 2.031965494155884\n",
            "Train loss at epoch 414 = 2.0139682292938232\n",
            "Train loss at epoch 415 = 2.0123722553253174\n",
            "Train loss at epoch 416 = 2.003403902053833\n",
            "Train loss at epoch 417 = 2.0003418922424316\n",
            "Train loss at epoch 418 = 2.026930332183838\n",
            "Train loss at epoch 419 = 2.03356671333313\n",
            "Train loss at epoch 420 = 1.9974350929260254\n",
            "Train loss at epoch 421 = 2.006131410598755\n",
            "Train loss at epoch 422 = 1.987321376800537\n",
            "Train loss at epoch 423 = 2.015610694885254\n",
            "Train loss at epoch 424 = 1.9901329278945923\n",
            "Train loss at epoch 425 = 1.991884708404541\n",
            "Train loss at epoch 426 = 1.986330270767212\n",
            "Train loss at epoch 427 = 2.0002431869506836\n",
            "Train loss at epoch 428 = 1.9888036251068115\n",
            "Train loss at epoch 429 = 2.0181217193603516\n",
            "Train loss at epoch 430 = 2.0247249603271484\n",
            "Train loss at epoch 431 = 1.9773070812225342\n",
            "Train loss at epoch 432 = 1.9634541273117065\n",
            "Train loss at epoch 433 = 2.011918067932129\n",
            "Train loss at epoch 434 = 1.9855316877365112\n",
            "Train loss at epoch 435 = 1.9656670093536377\n",
            "Train loss at epoch 436 = 1.981835126876831\n",
            "Train loss at epoch 437 = 1.9912598133087158\n",
            "Train loss at epoch 438 = 1.9891531467437744\n",
            "Train loss at epoch 439 = 1.9722316265106201\n",
            "Train loss at epoch 440 = 1.9807665348052979\n",
            "Train loss at epoch 441 = 1.9991743564605713\n",
            "Train loss at epoch 442 = 1.975797414779663\n",
            "Train loss at epoch 443 = 1.9947720766067505\n",
            "Train loss at epoch 444 = 1.9860821962356567\n",
            "Train loss at epoch 445 = 1.992751955986023\n",
            "Train loss at epoch 446 = 1.9886603355407715\n",
            "Train loss at epoch 447 = 1.958899736404419\n",
            "Train loss at epoch 448 = 1.9707831144332886\n",
            "Train loss at epoch 449 = 1.9863883256912231\n",
            "Train loss at epoch 450 = 1.9779174327850342\n",
            "Train loss at epoch 451 = 1.9933046102523804\n",
            "Train loss at epoch 452 = 1.9531892538070679\n",
            "Train loss at epoch 453 = 1.9641460180282593\n",
            "Train loss at epoch 454 = 1.976974368095398\n",
            "Train loss at epoch 455 = 1.9841285943984985\n",
            "Train loss at epoch 456 = 1.952129602432251\n",
            "Train loss at epoch 457 = 1.9897445440292358\n",
            "Train loss at epoch 458 = 1.9415738582611084\n",
            "Train loss at epoch 459 = 1.9620028734207153\n",
            "Train loss at epoch 460 = 1.9455665349960327\n",
            "Train loss at epoch 461 = 1.954037070274353\n",
            "Train loss at epoch 462 = 1.9976444244384766\n",
            "Train loss at epoch 463 = 1.9498080015182495\n",
            "Train loss at epoch 464 = 1.9583539962768555\n",
            "Train loss at epoch 465 = 1.9815776348114014\n",
            "Train loss at epoch 466 = 1.9461153745651245\n",
            "Train loss at epoch 467 = 1.965480923652649\n",
            "Train loss at epoch 468 = 1.9471948146820068\n",
            "Train loss at epoch 469 = 1.9337069988250732\n",
            "Train loss at epoch 470 = 1.945647954940796\n",
            "Train loss at epoch 471 = 1.9427577257156372\n",
            "Train loss at epoch 472 = 1.9335486888885498\n",
            "Train loss at epoch 473 = 1.951987862586975\n",
            "Train loss at epoch 474 = 1.9561865329742432\n",
            "Train loss at epoch 475 = 1.9426319599151611\n",
            "Train loss at epoch 476 = 1.9360727071762085\n",
            "Train loss at epoch 477 = 1.9285321235656738\n",
            "Train loss at epoch 478 = 1.9464651346206665\n",
            "Train loss at epoch 479 = 1.9673625230789185\n",
            "Train loss at epoch 480 = 1.93338942527771\n",
            "Train loss at epoch 481 = 1.9277751445770264\n",
            "Train loss at epoch 482 = 1.9245134592056274\n",
            "Train loss at epoch 483 = 1.9252440929412842\n",
            "Train loss at epoch 484 = 1.9650384187698364\n",
            "Train loss at epoch 485 = 1.9351969957351685\n",
            "Train loss at epoch 486 = 1.9137135744094849\n",
            "Train loss at epoch 487 = 1.9462518692016602\n",
            "Train loss at epoch 488 = 1.9253814220428467\n",
            "Train loss at epoch 489 = 1.973082184791565\n",
            "Train loss at epoch 490 = 1.9473384618759155\n",
            "Train loss at epoch 491 = 1.9122518301010132\n",
            "Train loss at epoch 492 = 1.9001747369766235\n",
            "Train loss at epoch 493 = 1.9307024478912354\n",
            "Train loss at epoch 494 = 1.9225252866744995\n",
            "Train loss at epoch 495 = 1.9314274787902832\n",
            "Train loss at epoch 496 = 1.920233964920044\n",
            "Train loss at epoch 497 = 1.932682991027832\n",
            "Train loss at epoch 498 = 1.904573678970337\n",
            "Train loss at epoch 499 = 1.9382133483886719\n",
            "Train loss at epoch 500 = 1.9275020360946655\n",
            "val loss = 2.0447616577148438\n",
            "Train loss at epoch 501 = 1.9192125797271729\n",
            "Train loss at epoch 502 = 1.908818006515503\n",
            "Train loss at epoch 503 = 1.9144212007522583\n",
            "Train loss at epoch 504 = 1.9087259769439697\n",
            "Train loss at epoch 505 = 1.9080824851989746\n",
            "Train loss at epoch 506 = 1.9289820194244385\n",
            "Train loss at epoch 507 = 1.9139076471328735\n",
            "Train loss at epoch 508 = 1.9088314771652222\n",
            "Train loss at epoch 509 = 1.8803997039794922\n",
            "Train loss at epoch 510 = 1.9213621616363525\n",
            "Train loss at epoch 511 = 1.9175728559494019\n",
            "Train loss at epoch 512 = 1.9167046546936035\n",
            "Train loss at epoch 513 = 1.8958407640457153\n",
            "Train loss at epoch 514 = 1.9232196807861328\n",
            "Train loss at epoch 515 = 1.908061146736145\n",
            "Train loss at epoch 516 = 1.8857746124267578\n",
            "Train loss at epoch 517 = 1.8755245208740234\n",
            "Train loss at epoch 518 = 1.866562008857727\n",
            "Train loss at epoch 519 = 1.9213422536849976\n",
            "Train loss at epoch 520 = 1.91957426071167\n",
            "Train loss at epoch 521 = 1.8647199869155884\n",
            "Train loss at epoch 522 = 1.8780136108398438\n",
            "Train loss at epoch 523 = 1.9096529483795166\n",
            "Train loss at epoch 524 = 1.8786612749099731\n",
            "Train loss at epoch 525 = 1.8872920274734497\n",
            "Train loss at epoch 526 = 1.9180818796157837\n",
            "Train loss at epoch 527 = 1.8759931325912476\n",
            "Train loss at epoch 528 = 1.9040310382843018\n",
            "Train loss at epoch 529 = 1.901201605796814\n",
            "Train loss at epoch 530 = 1.8732680082321167\n",
            "Train loss at epoch 531 = 1.9017457962036133\n",
            "Train loss at epoch 532 = 1.8698831796646118\n",
            "Train loss at epoch 533 = 1.8912066221237183\n",
            "Train loss at epoch 534 = 1.8951442241668701\n",
            "Train loss at epoch 535 = 1.8838837146759033\n",
            "Train loss at epoch 536 = 1.8674452304840088\n",
            "Train loss at epoch 537 = 1.8622864484786987\n",
            "Train loss at epoch 538 = 1.8703688383102417\n",
            "Train loss at epoch 539 = 1.8909034729003906\n",
            "Train loss at epoch 540 = 1.8266493082046509\n",
            "Train loss at epoch 541 = 1.8668254613876343\n",
            "Train loss at epoch 542 = 1.8907591104507446\n",
            "Train loss at epoch 543 = 1.8852344751358032\n",
            "Train loss at epoch 544 = 1.8790684938430786\n",
            "Train loss at epoch 545 = 1.889214038848877\n",
            "Train loss at epoch 546 = 1.882758378982544\n",
            "Train loss at epoch 547 = 1.862953782081604\n",
            "Train loss at epoch 548 = 1.8807622194290161\n",
            "Train loss at epoch 549 = 1.8827462196350098\n",
            "Train loss at epoch 550 = 1.8875579833984375\n",
            "Train loss at epoch 551 = 1.8659356832504272\n",
            "Train loss at epoch 552 = 1.8398942947387695\n",
            "Train loss at epoch 553 = 1.854011058807373\n",
            "Train loss at epoch 554 = 1.8317705392837524\n",
            "Train loss at epoch 555 = 1.8582450151443481\n",
            "Train loss at epoch 556 = 1.8905845880508423\n",
            "Train loss at epoch 557 = 1.846632719039917\n",
            "Train loss at epoch 558 = 1.8649559020996094\n",
            "Train loss at epoch 559 = 1.848222255706787\n",
            "Train loss at epoch 560 = 1.8227261304855347\n",
            "Train loss at epoch 561 = 1.8316932916641235\n",
            "Train loss at epoch 562 = 1.8574503660202026\n",
            "Train loss at epoch 563 = 1.849806785583496\n",
            "Train loss at epoch 564 = 1.8210409879684448\n",
            "Train loss at epoch 565 = 1.8544937372207642\n",
            "Train loss at epoch 566 = 1.8484121561050415\n",
            "Train loss at epoch 567 = 1.833113193511963\n",
            "Train loss at epoch 568 = 1.8664692640304565\n",
            "Train loss at epoch 569 = 1.847454309463501\n",
            "Train loss at epoch 570 = 1.8537135124206543\n",
            "Train loss at epoch 571 = 1.8492388725280762\n",
            "Train loss at epoch 572 = 1.8748393058776855\n",
            "Train loss at epoch 573 = 1.8328202962875366\n",
            "Train loss at epoch 574 = 1.8293817043304443\n",
            "Train loss at epoch 575 = 1.8565342426300049\n",
            "Train loss at epoch 576 = 1.8659883737564087\n",
            "Train loss at epoch 577 = 1.83504056930542\n",
            "Train loss at epoch 578 = 1.8731173276901245\n",
            "Train loss at epoch 579 = 1.8256709575653076\n",
            "Train loss at epoch 580 = 1.831640601158142\n",
            "Train loss at epoch 581 = 1.822846531867981\n",
            "Train loss at epoch 582 = 1.850195050239563\n",
            "Train loss at epoch 583 = 1.8492487668991089\n",
            "Train loss at epoch 584 = 1.841224193572998\n",
            "Train loss at epoch 585 = 1.8227391242980957\n",
            "Train loss at epoch 586 = 1.8495903015136719\n",
            "Train loss at epoch 587 = 1.8592259883880615\n",
            "Train loss at epoch 588 = 1.8295488357543945\n",
            "Train loss at epoch 589 = 1.844204068183899\n",
            "Train loss at epoch 590 = 1.8299322128295898\n",
            "Train loss at epoch 591 = 1.8005690574645996\n",
            "Train loss at epoch 592 = 1.8320362567901611\n",
            "Train loss at epoch 593 = 1.851401448249817\n",
            "Train loss at epoch 594 = 1.809869408607483\n",
            "Train loss at epoch 595 = 1.8602396249771118\n",
            "Train loss at epoch 596 = 1.7763234376907349\n",
            "Train loss at epoch 597 = 1.8270361423492432\n",
            "Train loss at epoch 598 = 1.8646701574325562\n",
            "Train loss at epoch 599 = 1.806580901145935\n",
            "Train loss at epoch 600 = 1.84056556224823\n",
            "Train loss at epoch 601 = 1.8428629636764526\n",
            "Train loss at epoch 602 = 1.849446177482605\n",
            "Train loss at epoch 603 = 1.874971628189087\n",
            "Train loss at epoch 604 = 1.8107830286026\n",
            "Train loss at epoch 605 = 1.8023227453231812\n",
            "Train loss at epoch 606 = 1.8309056758880615\n",
            "Train loss at epoch 607 = 1.8079146146774292\n",
            "Train loss at epoch 608 = 1.8347890377044678\n",
            "Train loss at epoch 609 = 1.827133297920227\n",
            "Train loss at epoch 610 = 1.7974095344543457\n",
            "Train loss at epoch 611 = 1.8010218143463135\n",
            "Train loss at epoch 612 = 1.8088778257369995\n",
            "Train loss at epoch 613 = 1.800420880317688\n",
            "Train loss at epoch 614 = 1.7923780679702759\n",
            "Train loss at epoch 615 = 1.804826259613037\n",
            "Train loss at epoch 616 = 1.791906714439392\n",
            "Train loss at epoch 617 = 1.7743501663208008\n",
            "Train loss at epoch 618 = 1.823861837387085\n",
            "Train loss at epoch 619 = 1.8319180011749268\n",
            "Train loss at epoch 620 = 1.7826021909713745\n",
            "Train loss at epoch 621 = 1.8305249214172363\n",
            "Train loss at epoch 622 = 1.811519980430603\n",
            "Train loss at epoch 623 = 1.8009610176086426\n",
            "Train loss at epoch 624 = 1.8234522342681885\n",
            "Train loss at epoch 625 = 1.8049557209014893\n",
            "Train loss at epoch 626 = 1.8361715078353882\n",
            "Train loss at epoch 627 = 1.830912709236145\n",
            "Train loss at epoch 628 = 1.8159294128417969\n",
            "Train loss at epoch 629 = 1.8022193908691406\n",
            "Train loss at epoch 630 = 1.8094359636306763\n",
            "Train loss at epoch 631 = 1.7959914207458496\n",
            "Train loss at epoch 632 = 1.7975329160690308\n",
            "Train loss at epoch 633 = 1.7808401584625244\n",
            "Train loss at epoch 634 = 1.8025157451629639\n",
            "Train loss at epoch 635 = 1.7871679067611694\n",
            "Train loss at epoch 636 = 1.807004451751709\n",
            "Train loss at epoch 637 = 1.789501428604126\n",
            "Train loss at epoch 638 = 1.7909231185913086\n",
            "Train loss at epoch 639 = 1.8079733848571777\n",
            "Train loss at epoch 640 = 1.7924240827560425\n",
            "Train loss at epoch 641 = 1.819202184677124\n",
            "Train loss at epoch 642 = 1.81423819065094\n",
            "Train loss at epoch 643 = 1.8123364448547363\n",
            "Train loss at epoch 644 = 1.7755376100540161\n",
            "Train loss at epoch 645 = 1.7764426469802856\n",
            "Train loss at epoch 646 = 1.7939331531524658\n",
            "Train loss at epoch 647 = 1.7843148708343506\n",
            "Train loss at epoch 648 = 1.8095836639404297\n",
            "Train loss at epoch 649 = 1.7751185894012451\n",
            "Train loss at epoch 650 = 1.8069648742675781\n",
            "Train loss at epoch 651 = 1.778955340385437\n",
            "Train loss at epoch 652 = 1.7999348640441895\n",
            "Train loss at epoch 653 = 1.784749150276184\n",
            "Train loss at epoch 654 = 1.7940027713775635\n",
            "Train loss at epoch 655 = 1.769896149635315\n",
            "Train loss at epoch 656 = 1.7966104745864868\n",
            "Train loss at epoch 657 = 1.80408775806427\n",
            "Train loss at epoch 658 = 1.808531641960144\n",
            "Train loss at epoch 659 = 1.7875885963439941\n",
            "Train loss at epoch 660 = 1.787150502204895\n",
            "Train loss at epoch 661 = 1.7824040651321411\n",
            "Train loss at epoch 662 = 1.764456868171692\n",
            "Train loss at epoch 663 = 1.7898157835006714\n",
            "Train loss at epoch 664 = 1.7819210290908813\n",
            "Train loss at epoch 665 = 1.7521048784255981\n",
            "Train loss at epoch 666 = 1.8075141906738281\n",
            "Train loss at epoch 667 = 1.7500287294387817\n",
            "Train loss at epoch 668 = 1.7957329750061035\n",
            "Train loss at epoch 669 = 1.784974217414856\n",
            "Train loss at epoch 670 = 1.8078562021255493\n",
            "Train loss at epoch 671 = 1.7517504692077637\n",
            "Train loss at epoch 672 = 1.7924610376358032\n",
            "Train loss at epoch 673 = 1.7590199708938599\n",
            "Train loss at epoch 674 = 1.774106502532959\n",
            "Train loss at epoch 675 = 1.7313412427902222\n",
            "Train loss at epoch 676 = 1.7729681730270386\n",
            "Train loss at epoch 677 = 1.7659167051315308\n",
            "Train loss at epoch 678 = 1.807984471321106\n",
            "Train loss at epoch 679 = 1.7496800422668457\n",
            "Train loss at epoch 680 = 1.748094081878662\n",
            "Train loss at epoch 681 = 1.7389066219329834\n",
            "Train loss at epoch 682 = 1.7607407569885254\n",
            "Train loss at epoch 683 = 1.8036984205245972\n",
            "Train loss at epoch 684 = 1.7650331258773804\n",
            "Train loss at epoch 685 = 1.759137749671936\n",
            "Train loss at epoch 686 = 1.760805368423462\n",
            "Train loss at epoch 687 = 1.7526791095733643\n",
            "Train loss at epoch 688 = 1.7614823579788208\n",
            "Train loss at epoch 689 = 1.7803207635879517\n",
            "Train loss at epoch 690 = 1.7522646188735962\n",
            "Train loss at epoch 691 = 1.7678569555282593\n",
            "Train loss at epoch 692 = 1.772089958190918\n",
            "Train loss at epoch 693 = 1.7642898559570312\n",
            "Train loss at epoch 694 = 1.7391760349273682\n",
            "Train loss at epoch 695 = 1.7643473148345947\n",
            "Train loss at epoch 696 = 1.7880388498306274\n",
            "Train loss at epoch 697 = 1.7361235618591309\n",
            "Train loss at epoch 698 = 1.742085337638855\n",
            "Train loss at epoch 699 = 1.7748743295669556\n",
            "Train loss at epoch 700 = 1.7589854001998901\n",
            "Train loss at epoch 701 = 1.708743929862976\n",
            "Train loss at epoch 702 = 1.7460743188858032\n",
            "Train loss at epoch 703 = 1.7487918138504028\n",
            "Train loss at epoch 704 = 1.7395399808883667\n",
            "Train loss at epoch 705 = 1.7490907907485962\n",
            "Train loss at epoch 706 = 1.7596179246902466\n",
            "Train loss at epoch 707 = 1.7750476598739624\n",
            "Train loss at epoch 708 = 1.7738046646118164\n",
            "Train loss at epoch 709 = 1.7354419231414795\n",
            "Train loss at epoch 710 = 1.756483554840088\n",
            "Train loss at epoch 711 = 1.775159478187561\n",
            "Train loss at epoch 712 = 1.7276655435562134\n",
            "Train loss at epoch 713 = 1.7478210926055908\n",
            "Train loss at epoch 714 = 1.7286324501037598\n",
            "Train loss at epoch 715 = 1.7452607154846191\n",
            "Train loss at epoch 716 = 1.7397704124450684\n",
            "Train loss at epoch 717 = 1.7500942945480347\n",
            "Train loss at epoch 718 = 1.7430557012557983\n",
            "Train loss at epoch 719 = 1.7480831146240234\n",
            "Train loss at epoch 720 = 1.7054972648620605\n",
            "Train loss at epoch 721 = 1.763673186302185\n",
            "Train loss at epoch 722 = 1.7473925352096558\n",
            "Train loss at epoch 723 = 1.739142656326294\n",
            "Train loss at epoch 724 = 1.7608327865600586\n",
            "Train loss at epoch 725 = 1.7350407838821411\n",
            "Train loss at epoch 726 = 1.7307217121124268\n",
            "Train loss at epoch 727 = 1.733284831047058\n",
            "Train loss at epoch 728 = 1.723836064338684\n",
            "Train loss at epoch 729 = 1.7518917322158813\n",
            "Train loss at epoch 730 = 1.7382735013961792\n",
            "Train loss at epoch 731 = 1.7486629486083984\n",
            "Train loss at epoch 732 = 1.7349752187728882\n",
            "Train loss at epoch 733 = 1.7488417625427246\n",
            "Train loss at epoch 734 = 1.7156704664230347\n",
            "Train loss at epoch 735 = 1.7479658126831055\n",
            "Train loss at epoch 736 = 1.7677538394927979\n",
            "Train loss at epoch 737 = 1.7361489534378052\n",
            "Train loss at epoch 738 = 1.7370160818099976\n",
            "Train loss at epoch 739 = 1.717130184173584\n",
            "Train loss at epoch 740 = 1.7388484477996826\n",
            "Train loss at epoch 741 = 1.7504887580871582\n",
            "Train loss at epoch 742 = 1.7194842100143433\n",
            "Train loss at epoch 743 = 1.736806869506836\n",
            "Train loss at epoch 744 = 1.7378082275390625\n",
            "Train loss at epoch 745 = 1.7517532110214233\n",
            "Train loss at epoch 746 = 1.746376633644104\n",
            "Train loss at epoch 747 = 1.7471874952316284\n",
            "Train loss at epoch 748 = 1.7269785404205322\n",
            "Train loss at epoch 749 = 1.7048242092132568\n",
            "Train loss at epoch 750 = 1.7256885766983032\n",
            "Train loss at epoch 751 = 1.7370576858520508\n",
            "Train loss at epoch 752 = 1.68904447555542\n",
            "Train loss at epoch 753 = 1.7171436548233032\n",
            "Train loss at epoch 754 = 1.6995023488998413\n",
            "Train loss at epoch 755 = 1.7309755086898804\n",
            "Train loss at epoch 756 = 1.7312792539596558\n",
            "Train loss at epoch 757 = 1.7260668277740479\n",
            "Train loss at epoch 758 = 1.684529185295105\n",
            "Train loss at epoch 759 = 1.6996355056762695\n",
            "Train loss at epoch 760 = 1.7496815919876099\n",
            "Train loss at epoch 761 = 1.7167646884918213\n",
            "Train loss at epoch 762 = 1.713730812072754\n",
            "Train loss at epoch 763 = 1.7289917469024658\n",
            "Train loss at epoch 764 = 1.7338752746582031\n",
            "Train loss at epoch 765 = 1.6960328817367554\n",
            "Train loss at epoch 766 = 1.698055386543274\n",
            "Train loss at epoch 767 = 1.686021327972412\n",
            "Train loss at epoch 768 = 1.6745370626449585\n",
            "Train loss at epoch 769 = 1.7211289405822754\n",
            "Train loss at epoch 770 = 1.7058320045471191\n",
            "Train loss at epoch 771 = 1.7131575345993042\n",
            "Train loss at epoch 772 = 1.7080949544906616\n",
            "Train loss at epoch 773 = 1.7172553539276123\n",
            "Train loss at epoch 774 = 1.6902191638946533\n",
            "Train loss at epoch 775 = 1.6893742084503174\n",
            "Train loss at epoch 776 = 1.744958758354187\n",
            "Train loss at epoch 777 = 1.7454553842544556\n",
            "Train loss at epoch 778 = 1.709975004196167\n",
            "Train loss at epoch 779 = 1.7179919481277466\n",
            "Train loss at epoch 780 = 1.7193920612335205\n",
            "Train loss at epoch 781 = 1.6735260486602783\n",
            "Train loss at epoch 782 = 1.7166719436645508\n",
            "Train loss at epoch 783 = 1.7096914052963257\n",
            "Train loss at epoch 784 = 1.6985689401626587\n",
            "Train loss at epoch 785 = 1.7187772989273071\n",
            "Train loss at epoch 786 = 1.6974951028823853\n",
            "Train loss at epoch 787 = 1.704683780670166\n",
            "Train loss at epoch 788 = 1.671898603439331\n",
            "Train loss at epoch 789 = 1.7022559642791748\n",
            "Train loss at epoch 790 = 1.7215107679367065\n",
            "Train loss at epoch 791 = 1.7357391119003296\n",
            "Train loss at epoch 792 = 1.653092384338379\n",
            "Train loss at epoch 793 = 1.7302457094192505\n",
            "Train loss at epoch 794 = 1.7181462049484253\n",
            "Train loss at epoch 795 = 1.6917293071746826\n",
            "Train loss at epoch 796 = 1.6983128786087036\n",
            "Train loss at epoch 797 = 1.6951218843460083\n",
            "Train loss at epoch 798 = 1.6702690124511719\n",
            "Train loss at epoch 799 = 1.6923446655273438\n",
            "Train loss at epoch 800 = 1.6869393587112427\n",
            "Train loss at epoch 801 = 1.6926534175872803\n",
            "Train loss at epoch 802 = 1.6951862573623657\n",
            "Train loss at epoch 803 = 1.733258605003357\n",
            "Train loss at epoch 804 = 1.705905556678772\n",
            "Train loss at epoch 805 = 1.7265759706497192\n",
            "Train loss at epoch 806 = 1.6946659088134766\n",
            "Train loss at epoch 807 = 1.7163690328598022\n",
            "Train loss at epoch 808 = 1.6921205520629883\n",
            "Train loss at epoch 809 = 1.7137165069580078\n",
            "Train loss at epoch 810 = 1.702311396598816\n",
            "Train loss at epoch 811 = 1.7150280475616455\n",
            "Train loss at epoch 812 = 1.6695125102996826\n",
            "Train loss at epoch 813 = 1.685305118560791\n",
            "Train loss at epoch 814 = 1.6982433795928955\n",
            "Train loss at epoch 815 = 1.6770848035812378\n",
            "Train loss at epoch 816 = 1.7110716104507446\n",
            "Train loss at epoch 817 = 1.6623090505599976\n",
            "Train loss at epoch 818 = 1.7171581983566284\n",
            "Train loss at epoch 819 = 1.6656262874603271\n",
            "Train loss at epoch 820 = 1.7092766761779785\n",
            "Train loss at epoch 821 = 1.7010836601257324\n",
            "Train loss at epoch 822 = 1.6978793144226074\n",
            "Train loss at epoch 823 = 1.6899521350860596\n",
            "Train loss at epoch 824 = 1.7029293775558472\n",
            "Train loss at epoch 825 = 1.7126805782318115\n",
            "Train loss at epoch 826 = 1.7010606527328491\n",
            "Train loss at epoch 827 = 1.697493314743042\n",
            "Train loss at epoch 828 = 1.7019855976104736\n",
            "Train loss at epoch 829 = 1.704730749130249\n",
            "Train loss at epoch 830 = 1.724616289138794\n",
            "Train loss at epoch 831 = 1.6977412700653076\n",
            "Train loss at epoch 832 = 1.7007423639297485\n",
            "Train loss at epoch 833 = 1.7218401432037354\n",
            "Train loss at epoch 834 = 1.6934680938720703\n",
            "Train loss at epoch 835 = 1.698967695236206\n",
            "Train loss at epoch 836 = 1.6705862283706665\n",
            "Train loss at epoch 837 = 1.6907917261123657\n",
            "Train loss at epoch 838 = 1.6710093021392822\n",
            "Train loss at epoch 839 = 1.6947576999664307\n",
            "Train loss at epoch 840 = 1.6817922592163086\n",
            "Train loss at epoch 841 = 1.7205513715744019\n",
            "Train loss at epoch 842 = 1.7075729370117188\n",
            "Train loss at epoch 843 = 1.6515098810195923\n",
            "Train loss at epoch 844 = 1.6658564805984497\n",
            "Train loss at epoch 845 = 1.6675254106521606\n",
            "Train loss at epoch 846 = 1.6946396827697754\n",
            "Train loss at epoch 847 = 1.6862361431121826\n",
            "Train loss at epoch 848 = 1.701187252998352\n",
            "Train loss at epoch 849 = 1.6701502799987793\n",
            "Train loss at epoch 850 = 1.6821444034576416\n",
            "Train loss at epoch 851 = 1.7056281566619873\n",
            "Train loss at epoch 852 = 1.6562556028366089\n",
            "Train loss at epoch 853 = 1.6759183406829834\n",
            "Train loss at epoch 854 = 1.6821725368499756\n",
            "Train loss at epoch 855 = 1.687401294708252\n",
            "Train loss at epoch 856 = 1.685699224472046\n",
            "Train loss at epoch 857 = 1.6623610258102417\n",
            "Train loss at epoch 858 = 1.671872854232788\n",
            "Train loss at epoch 859 = 1.6375383138656616\n",
            "Train loss at epoch 860 = 1.685730218887329\n",
            "Train loss at epoch 861 = 1.6578513383865356\n",
            "Train loss at epoch 862 = 1.6557910442352295\n",
            "Train loss at epoch 863 = 1.6275999546051025\n",
            "Train loss at epoch 864 = 1.6921775341033936\n",
            "Train loss at epoch 865 = 1.701651930809021\n",
            "Train loss at epoch 866 = 1.6812634468078613\n",
            "Train loss at epoch 867 = 1.652839183807373\n",
            "Train loss at epoch 868 = 1.6741440296173096\n",
            "Train loss at epoch 869 = 1.6550172567367554\n",
            "Train loss at epoch 870 = 1.647640585899353\n",
            "Train loss at epoch 871 = 1.6483099460601807\n",
            "Train loss at epoch 872 = 1.656814694404602\n",
            "Train loss at epoch 873 = 1.6634565591812134\n",
            "Train loss at epoch 874 = 1.685766577720642\n",
            "Train loss at epoch 875 = 1.6553006172180176\n",
            "Train loss at epoch 876 = 1.6492741107940674\n",
            "Train loss at epoch 877 = 1.6662681102752686\n",
            "Train loss at epoch 878 = 1.6752105951309204\n",
            "Train loss at epoch 879 = 1.6631526947021484\n",
            "Train loss at epoch 880 = 1.6726938486099243\n",
            "Train loss at epoch 881 = 1.6487611532211304\n",
            "Train loss at epoch 882 = 1.6668113470077515\n",
            "Train loss at epoch 883 = 1.6700749397277832\n",
            "Train loss at epoch 884 = 1.6527314186096191\n",
            "Train loss at epoch 885 = 1.6457257270812988\n",
            "Train loss at epoch 886 = 1.6705185174942017\n",
            "Train loss at epoch 887 = 1.6816200017929077\n",
            "Train loss at epoch 888 = 1.6870695352554321\n",
            "Train loss at epoch 889 = 1.6623221635818481\n",
            "Train loss at epoch 890 = 1.6217395067214966\n",
            "Train loss at epoch 891 = 1.665515661239624\n",
            "Train loss at epoch 892 = 1.6636139154434204\n",
            "Train loss at epoch 893 = 1.664257526397705\n",
            "Train loss at epoch 894 = 1.6713790893554688\n",
            "Train loss at epoch 895 = 1.644811749458313\n",
            "Train loss at epoch 896 = 1.6693527698516846\n",
            "Train loss at epoch 897 = 1.6682771444320679\n",
            "Train loss at epoch 898 = 1.655544638633728\n",
            "Train loss at epoch 899 = 1.6447889804840088\n",
            "Train loss at epoch 900 = 1.6658672094345093\n",
            "Train loss at epoch 901 = 1.6816368103027344\n",
            "Train loss at epoch 902 = 1.6446503400802612\n",
            "Train loss at epoch 903 = 1.6695687770843506\n",
            "Train loss at epoch 904 = 1.6449041366577148\n",
            "Train loss at epoch 905 = 1.6678105592727661\n",
            "Train loss at epoch 906 = 1.6760073900222778\n",
            "Train loss at epoch 907 = 1.6398940086364746\n",
            "Train loss at epoch 908 = 1.636684775352478\n",
            "Train loss at epoch 909 = 1.65787935256958\n",
            "Train loss at epoch 910 = 1.6807644367218018\n",
            "Train loss at epoch 911 = 1.6462279558181763\n",
            "Train loss at epoch 912 = 1.6465328931808472\n",
            "Train loss at epoch 913 = 1.6219338178634644\n",
            "Train loss at epoch 914 = 1.6311973333358765\n",
            "Train loss at epoch 915 = 1.6591085195541382\n",
            "Train loss at epoch 916 = 1.6881706714630127\n",
            "Train loss at epoch 917 = 1.6498370170593262\n",
            "Train loss at epoch 918 = 1.6407440900802612\n",
            "Train loss at epoch 919 = 1.6260672807693481\n",
            "Train loss at epoch 920 = 1.6431106328964233\n",
            "Train loss at epoch 921 = 1.6404443979263306\n",
            "Train loss at epoch 922 = 1.6225882768630981\n",
            "Train loss at epoch 923 = 1.6689302921295166\n",
            "Train loss at epoch 924 = 1.6761984825134277\n",
            "Train loss at epoch 925 = 1.6200295686721802\n",
            "Train loss at epoch 926 = 1.6668421030044556\n",
            "Train loss at epoch 927 = 1.6423530578613281\n",
            "Train loss at epoch 928 = 1.6340351104736328\n",
            "Train loss at epoch 929 = 1.6507481336593628\n",
            "Train loss at epoch 930 = 1.634626030921936\n",
            "Train loss at epoch 931 = 1.6319553852081299\n",
            "Train loss at epoch 932 = 1.6722489595413208\n",
            "Train loss at epoch 933 = 1.6330108642578125\n",
            "Train loss at epoch 934 = 1.6608346700668335\n",
            "Train loss at epoch 935 = 1.6657066345214844\n",
            "Train loss at epoch 936 = 1.6573389768600464\n",
            "Train loss at epoch 937 = 1.632386565208435\n",
            "Train loss at epoch 938 = 1.6386390924453735\n",
            "Train loss at epoch 939 = 1.6207157373428345\n",
            "Train loss at epoch 940 = 1.6476185321807861\n",
            "Train loss at epoch 941 = 1.6457438468933105\n",
            "Train loss at epoch 942 = 1.6334174871444702\n",
            "Train loss at epoch 943 = 1.6466280221939087\n",
            "Train loss at epoch 944 = 1.6570308208465576\n",
            "Train loss at epoch 945 = 1.6058012247085571\n",
            "Train loss at epoch 946 = 1.6307175159454346\n",
            "Train loss at epoch 947 = 1.6618614196777344\n",
            "Train loss at epoch 948 = 1.6281436681747437\n",
            "Train loss at epoch 949 = 1.6474392414093018\n",
            "Train loss at epoch 950 = 1.6207481622695923\n",
            "Train loss at epoch 951 = 1.6479027271270752\n",
            "Train loss at epoch 952 = 1.6057853698730469\n",
            "Train loss at epoch 953 = 1.6118953227996826\n",
            "Train loss at epoch 954 = 1.6567484140396118\n",
            "Train loss at epoch 955 = 1.646751880645752\n",
            "Train loss at epoch 956 = 1.612954020500183\n",
            "Train loss at epoch 957 = 1.676365852355957\n",
            "Train loss at epoch 958 = 1.6489171981811523\n",
            "Train loss at epoch 959 = 1.6101728677749634\n",
            "Train loss at epoch 960 = 1.6199043989181519\n",
            "Train loss at epoch 961 = 1.6058502197265625\n",
            "Train loss at epoch 962 = 1.6249845027923584\n",
            "Train loss at epoch 963 = 1.6058647632598877\n",
            "Train loss at epoch 964 = 1.6169333457946777\n",
            "Train loss at epoch 965 = 1.6293343305587769\n",
            "Train loss at epoch 966 = 1.6182953119277954\n",
            "Train loss at epoch 967 = 1.6099331378936768\n",
            "Train loss at epoch 968 = 1.6527605056762695\n",
            "Train loss at epoch 969 = 1.633987307548523\n",
            "Train loss at epoch 970 = 1.6535251140594482\n",
            "Train loss at epoch 971 = 1.643271803855896\n",
            "Train loss at epoch 972 = 1.63947594165802\n",
            "Train loss at epoch 973 = 1.643032431602478\n",
            "Train loss at epoch 974 = 1.640706181526184\n",
            "Train loss at epoch 975 = 1.6304441690444946\n",
            "Train loss at epoch 976 = 1.6353152990341187\n",
            "Train loss at epoch 977 = 1.615231990814209\n",
            "Train loss at epoch 978 = 1.619441032409668\n",
            "Train loss at epoch 979 = 1.6555203199386597\n",
            "Train loss at epoch 980 = 1.6201817989349365\n",
            "Train loss at epoch 981 = 1.6360183954238892\n",
            "Train loss at epoch 982 = 1.6226344108581543\n",
            "Train loss at epoch 983 = 1.6451183557510376\n",
            "Train loss at epoch 984 = 1.6401033401489258\n",
            "Train loss at epoch 985 = 1.6385127305984497\n",
            "Train loss at epoch 986 = 1.6259675025939941\n",
            "Train loss at epoch 987 = 1.6307214498519897\n",
            "Train loss at epoch 988 = 1.620557427406311\n",
            "Train loss at epoch 989 = 1.642635464668274\n",
            "Train loss at epoch 990 = 1.6130317449569702\n",
            "Train loss at epoch 991 = 1.584244728088379\n",
            "Train loss at epoch 992 = 1.6239886283874512\n",
            "Train loss at epoch 993 = 1.6638233661651611\n",
            "Train loss at epoch 994 = 1.6086677312850952\n",
            "Train loss at epoch 995 = 1.6359076499938965\n",
            "Train loss at epoch 996 = 1.6276121139526367\n",
            "Train loss at epoch 997 = 1.635672926902771\n",
            "Train loss at epoch 998 = 1.610564112663269\n",
            "Train loss at epoch 999 = 1.6368077993392944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id = torch.zeros((1,1), dtype= torch.long)\n",
        "print(decode(m.generate(id,max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O62ROfpT7nUb",
        "outputId": "abccd9d1-2512-4468-ac5f-605aaf3c281a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lord.\n",
            "\n",
            "QUEEN XMELIA: no hath I pisker, yet to\n",
            "the have carr'd tellow, got to good!'\n",
            "\n",
            "SLEOR:\n",
            "That nen\n"
          ]
        }
      ]
    }
  ]
}